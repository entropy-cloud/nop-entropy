import requests
import json
import re
from typing import List, Dict
from pathlib import Path
from datetime import datetime


class RoughChunker:
    def __init__(self, max_size: int = 8000):
        self.max_size = max_size
    
    def chunk(self, content: str) -> List[str]:
        chunks = []
        current_chunk = ""
        in_code_block = False
        
        lines = content.split('\n')
        i = 0
        
        while i < len(lines):
            line = lines[i]
            
            if line.startswith('```'):
                in_code_block = not in_code_block
                current_chunk += line + '\n'
                i += 1
                continue
            
            if line.startswith('## '):
                if current_chunk and len(current_chunk) > 1000:
                    chunks.append(current_chunk.strip())
                    current_chunk = line + '\n'
                else:
                    current_chunk += line + '\n'
            elif line.startswith('### '):
                if len(current_chunk) > 6000:
                    chunks.append(current_chunk.strip())
                    current_chunk = line + '\n'
                else:
                    current_chunk += line + '\n'
            else:
                current_chunk += line + '\n'
            
            if len(current_chunk) >= self.max_size:
                chunks.append(current_chunk.strip())
                current_chunk = ""
            
            i += 1
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks


class AIAnalyzer:
    def __init__(self, model: str = "glm-4.7-flash", base_url: str = "http://localhost:11434", timeout: int = 600, debug_dir: str = None):
        self.model = model
        self.base_url = base_url
        self.api_url = f"{base_url}/api/generate"
        self.timeout = timeout
        self.debug_dir = debug_dir
        self.call_count = 0
        
        if self.debug_dir:
            self.debug_path = Path(debug_dir)
            self.debug_path.mkdir(parents=True, exist_ok=True)
            self.prompt_file = self.debug_path / f"ai_prompts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            self.response_file = self.debug_path / f"ai_responses_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        else:
            self.prompt_file = None
            self.response_file = None
    
    def analyze_chunk(self, chunk: str, doc_type: str = "unknown") -> Dict:
        self.call_count += 1
        call_id = self.call_count
        
        prompt = self._build_prompt(chunk, doc_type)
        
        if self.prompt_file:
            with open(self.prompt_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{'='*80}\n")
                f.write(f"è°ƒç”¨ #{call_id} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ–‡æ¡£ç±»å‹: {doc_type}\n")
                f.write(f"å†…å®¹é•¿åº¦: {len(chunk)} å­—ç¬¦\n")
                f.write(f"{'='*80}\n\n")
                f.write(prompt)
                f.write(f"\n{'='*80}\n\n")
        
        try:
            response = requests.post(
                self.api_url,
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "temperature": 0.3,
                    "max_tokens": 8192
                },
                timeout=self.timeout
            )
            
            if response.status_code != 200:
                raise Exception(f"AIåˆ†æå¤±è´¥: {response.text}")
            
            result = response.json()
            analysis = result.get("response", "")
            
            if self.response_file:
                with open(self.response_file, 'a', encoding='utf-8') as f:
                    f.write(f"\n{'='*80}\n")
                    f.write(f"è°ƒç”¨ #{call_id} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"çŠ¶æ€ç : {response.status_code}\n")
                    f.write(f"{'='*80}\n\n")
                    f.write(analysis)
                    f.write(f"\n{'='*80}\n\n")
            
            return self._parse_analysis(analysis)
        except Exception as e:
            print(f"  âŒ AIåˆ†æå¤±è´¥: {e}")
            
            if self.response_file:
                with open(self.response_file, 'a', encoding='utf-8') as f:
                    f.write(f"\n{'='*80}\n")
                    f.write(f"è°ƒç”¨ #{call_id} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"é”™è¯¯: {e}\n")
                    f.write(f"{'='*80}\n\n")
            
            return {
                "split_points": [],
                "chunks": [],
                "analysis": "è§£æå¤±è´¥"
            }
    
    def _build_prompt(self, chunk: str, doc_type: str) -> str:
        lines = chunk.split('\n')
        numbered_lines = []
        for i, line in enumerate(lines, 1):
            numbered_lines.append(f"{i}: {line}")
        numbered_chunk = '\n'.join(numbered_lines)
        
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ‡åˆ†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç»™å®šçš„æ–‡æ¡£å†…å®¹åˆ‡åˆ†æˆå¤šä¸ªè¯­ä¹‰å®Œæ•´çš„chunksã€‚

## åˆ‡åˆ†è¦æ±‚

1. **ä»£ç å—å®Œæ•´æ€§**ï¼šç¡®ä¿æ¯ä¸ªä»£ç å—å®Œæ•´ï¼Œä¸è¢«åˆ‡æ–­
2. **è¯­ä¹‰å•å…ƒå®Œæ•´**ï¼šä¿æŒæ–¹æ³•ã€ç±»ã€é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§
3. **ä¸Šä¸‹æ–‡ä¿ç•™**ï¼šä¿ç•™è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾¿äºç†è§£
4. **å¤§å°é€‚ä¸­**ï¼šå•ä¸ªchunkåœ¨3000å­—ç¬¦ä»¥å†…

## æ–‡æ¡£ç±»å‹

{doc_type}

## åˆ‡åˆ†ç­–ç•¥

- æ•™ç¨‹ç±»æ–‡æ¡£ï¼šæŒ‰æ­¥éª¤åˆ‡åˆ†
- ç¤ºä¾‹ç±»æ–‡æ¡£ï¼šæŒ‰ä»£ç å— + æ–¹æ³•è¾¹ç•Œåˆ‡åˆ†
- APIå‚è€ƒæ–‡æ¡£ï¼šæŒ‰æ¦‚å¿µåˆ‡åˆ†
- å®æˆ˜é¡¹ç›®æ–‡æ¡£ï¼šæŒ‰æ¨¡å— + ä»£ç å—åˆ‡åˆ†

## å¾…åˆ‡åˆ†å†…å®¹

æ³¨æ„ï¼šæ¯è¡Œå‰é¢å·²ç»æ ‡æ³¨äº†è¡Œå·ï¼ˆæ ¼å¼ï¼šè¡Œå·: å†…å®¹ï¼‰ï¼Œè¯·åœ¨è¾“å‡ºåˆ‡åˆ†å»ºè®®æ—¶ä½¿ç”¨è¿™äº›è¡Œå·ã€‚

```
{numbered_chunk}
```

## è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ‡åˆ†å»ºè®®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```json
{{
  
  "chunks": [
    {{
      "start_line": 1,
      "end_line": 10,
    }}
  ],
  "analysis": "åˆ‡åˆ†è¯´æ˜..."
}}
```

è¯·ç¡®ä¿è¾“å‡ºçš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå¹¶ä¸”line_numberä½¿ç”¨ä¸Šé¢æ ‡æ³¨çš„è¡Œå·ã€‚"""
    
    def _parse_analysis(self, analysis: str) -> Dict:
        try:
            json_start = analysis.find('{')
            json_end = analysis.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                raise Exception("æ— æ³•æ‰¾åˆ°JSONæ ¼å¼çš„è¾“å‡º")
            
            json_str = analysis[json_start:json_end]
            return json.loads(json_str)
        except Exception as e:
            print(f"  âš ï¸ è§£æAIåˆ†æç»“æœå¤±è´¥: {e}")
            print(f"  åŸå§‹è¾“å‡º: {analysis[:200]}...")
            return {
                "split_points": [],
                "chunks": [],
                "analysis": "è§£æå¤±è´¥"
            }


class FineChunker:
    def __init__(self, overlap_size: int = 150):
        self.overlap_size = overlap_size
    
    def chunk(self, content: str, split_points: List[Dict]) -> List[str]:
        if not split_points:
            return [content]
        
        lines = content.split('\n')
        chunks = []
        start_line = 0
        
        for split_point in split_points:
            end_line = split_point['line_number']
            
            if end_line > start_line:
                chunk = '\n'.join(lines[start_line:end_line])
                chunks.append(chunk.strip())
                start_line = end_line
        
        if start_line < len(lines):
            chunk = '\n'.join(lines[start_line:])
            chunks.append(chunk.strip())
        
        chunks = self._add_overlap(chunks)
        chunks = self._enforce_size(chunks)
        
        return chunks
    
    def _add_overlap(self, chunks: List[str]) -> List[str]:
        if len(chunks) <= 1:
            return chunks
        
        result = [chunks[0]]
        
        for i in range(1, len(chunks)):
            overlap = self._select_overlap(chunks[i-1])
            result.append(overlap + '\n' + chunks[i])
        
        return result
    
    def _select_overlap(self, chunk: str) -> str:
        if len(chunk) <= self.overlap_size:
            return chunk
        
        lines = chunk.split('\n')
        overlap_lines = []
        overlap_length = 0
        
        for line in reversed(lines):
            if overlap_length + len(line) > self.overlap_size:
                break
            overlap_lines.insert(0, line)
            overlap_length += len(line)
        
        return '\n'.join(overlap_lines)
    
    def _enforce_size(self, chunks: List[str], min_size: int = 200, max_size: int = 3000) -> List[str]:
        result = []
        for chunk in chunks:
            if len(chunk) < min_size and result:
                result[-1] += '\n' + chunk
            elif len(chunk) > max_size:
                result.extend(self._split_large_chunk(chunk, max_size))
            else:
                result.append(chunk)
        return result
    
    def _split_large_chunk(self, chunk: str, max_size: int) -> List[str]:
        chunks = []
        current_chunk = ""
        
        for line in chunk.split('\n'):
            if len(current_chunk) + len(line) > max_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = line + '\n'
            else:
                current_chunk += line + '\n'
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks


class QualityValidator:
    def __init__(self):
        pass
    
    def validate(self, chunks: List[str]) -> Dict:
        metrics = {
            "total_chunks": len(chunks),
            "broken_code_blocks": 0,
            "total_code_blocks": 0,
            "sizes": [],
            "min_size": float('inf'),
            "max_size": 0,
            "avg_size": 0
        }
        
        for chunk in chunks:
            size = len(chunk)
            metrics["sizes"].append(size)
            metrics["min_size"] = min(metrics["min_size"], size)
            metrics["max_size"] = max(metrics["max_size"], size)
            
            code_blocks = chunk.count('```')
            metrics["total_code_blocks"] += code_blocks
            
            if code_blocks % 2 != 0:
                metrics["broken_code_blocks"] += 1
        
        if metrics["sizes"]:
            metrics["avg_size"] = sum(metrics["sizes"]) / len(metrics["sizes"])
        
        metrics["code_block_break_rate"] = (
            metrics["broken_code_blocks"] / metrics["total_chunks"] * 100
            if metrics["total_chunks"] > 0
            else 0
        )
        
        metrics["semantic_completeness"] = (
            100 - metrics["code_block_break_rate"]
        )
        
        return metrics
    
    def generate_report(self, metrics: Dict) -> str:
        report = f"""
## åˆ‡åˆ†è´¨é‡æŠ¥å‘Š

### åŸºæœ¬ä¿¡æ¯
- æ€»chunksæ•°: {metrics['total_chunks']}
- å¹³å‡å¤§å°: {metrics['avg_size']:.0f} å­—ç¬¦
- æœ€å°å¤§å°: {metrics['min_size']:.0f} å­—ç¬¦
- æœ€å¤§å¤§å°: {metrics['max_size']:.0f} å­—ç¬¦

### ä»£ç å—å®Œæ•´æ€§
- æ€»ä»£ç å—æ•°: {metrics['total_code_blocks']}
- è¢«åˆ‡æ–­çš„ä»£ç å—: {metrics['broken_code_blocks']}
- ä»£ç å—åˆ‡æ–­ç‡: {metrics['code_block_break_rate']:.1f}%

### è¯­ä¹‰å®Œæ•´æ€§
- è¯­ä¹‰å®Œæ•´æ€§: {metrics['semantic_completeness']:.1f}%

### è¯„ä»·
"""
        if metrics['code_block_break_rate'] == 0:
            report += "âœ… ä¼˜ç§€ï¼šæ‰€æœ‰ä»£ç å—éƒ½å®Œæ•´\n"
        elif metrics['code_block_break_rate'] < 5:
            report += "âœ… è‰¯å¥½ï¼šä»£ç å—åˆ‡æ–­ç‡å¾ˆä½\n"
        elif metrics['code_block_break_rate'] < 10:
            report += "âš ï¸ ä¸€èˆ¬ï¼šä»£ç å—åˆ‡æ–­ç‡è¾ƒé«˜\n"
        else:
            report += "âŒ è¾ƒå·®ï¼šä»£ç å—åˆ‡æ–­ç‡è¿‡é«˜\n"
        
        return report


class AIChunker:
    def __init__(self, 
                 model: str = "glm-4.7-flash",
                 base_url: str = "http://localhost:11434",
                 rough_max_size: int = 8000,
                 overlap_size: int = 150,
                 timeout: int = 120,
                 debug_dir: str = None):
        self.rough_chunker = RoughChunker(max_size=rough_max_size)
        self.ai_analyzer = AIAnalyzer(model=model, base_url=base_url, timeout=timeout, debug_dir=debug_dir)
        self.fine_chunker = FineChunker(overlap_size=overlap_size)
        self.validator = QualityValidator()
    
    def chunk(self, content: str, doc_type: str = "unknown") -> List[str]:
        print(f"ğŸ” å¼€å§‹ç²—åˆ‡åˆ†...")
        rough_chunks = self.rough_chunker.chunk(content)
        print(f"âœ… ç²—åˆ‡åˆ†å®Œæˆï¼Œç”Ÿæˆ {len(rough_chunks)} ä¸ªchunks")
        
        final_chunks = []
        
        for i, rough_chunk in enumerate(rough_chunks, 1):
            print(f"ğŸ¤– AIåˆ†æ chunk {i}/{len(rough_chunks)}...")
            
            analysis = self.ai_analyzer.analyze_chunk(rough_chunk, doc_type)
            
            if analysis.get("chunks"):
                print(f"  âœ… AIå»ºè®®åˆ‡åˆ†ä¸º {len(analysis['chunks'])} ä¸ªchunks")
                for chunk_data in analysis["chunks"]:
                    final_chunks.append(chunk_data["content"])
            else:
                print(f"  âš ï¸ AIæœªæä¾›åˆ‡åˆ†å»ºè®®ï¼Œä½¿ç”¨åŸchunk")
                final_chunks.append(rough_chunk)
        
        print(f"âœ… ç»†åˆ‡åˆ†å®Œæˆï¼Œç”Ÿæˆ {len(final_chunks)} ä¸ªchunks")
        
        print(f"ğŸ” è´¨é‡éªŒè¯...")
        metrics = self.validator.validate(final_chunks)
        report = self.validator.generate_report(metrics)
        print(report)
        
        return final_chunks
    
    def chunk_file(self, input_path: str, output_path: str, doc_type: str = "unknown"):
        with open(input_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        chunks = self.chunk(content, doc_type)
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for i, chunk in enumerate(chunks, 1):
                chunk_data = {
                    "id": i,
                    "content": chunk,
                    "size": len(chunk)
                }
                f.write(json.dumps(chunk_data, ensure_ascii=False) + "\n")
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {output_path}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python ai_chunker.py <è¾“å…¥æ–‡ä»¶> <è¾“å‡ºæ–‡ä»¶> [æ–‡æ¡£ç±»å‹]")
        print("æ–‡æ¡£ç±»å‹: tutorial, example, api_reference, project")
        sys.exit(1)
    
    input_path = sys.argv[1]
    output_path = sys.argv[2]
    doc_type = sys.argv[3] if len(sys.argv) > 3 else "unknown"
    
    chunker = AIChunker()
    chunker.chunk_file(input_path, output_path, doc_type)
