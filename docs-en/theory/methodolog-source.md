# Methodological Sources of Reversible Computation

I have long harbored deep skepticism toward the multitude of design principles and so-called methodologies in software. Why do these principles exist, and can their effectiveness be proven scientifically? The theory of Reversible Computation seeks to provide a more solid theoretical foundation for software construction. It explicitly proposes elevating Delta to a first-principle concept and regarding the “total” as a special case of Delta—namely, as the difference between the identity element and the total (full = identity element + full)—reconstructing the entire conceptual system of the field around the concept of Delta. The intellectual origin of Reversible Computation is not computer science itself but theoretical physics. It regards software as an abstract entity in continuous evolution, described by different operational rules at different levels of complexity, and focuses on how tiny Deltas generated during evolution propagate through the system in an orderly way and interact. This article introduces the inspirations Reversible Computation draws from its sources in statistical physics and quantum mechanics.

(i) The Principle of Entropy Increase
In Shannon’s original definition, information is that which can reduce uncertainty. In physics, uncertainty corresponds to the disorder of a system, and the physical quantity that measures disorder is entropy. Upon receiving information, uncertainty decreases, which means the level of disorder decreases; hence information is considered negative entropy (negentropy).

Entropy is a core concept in physics, arguably even more important than energy. The first law of thermodynamics is the conservation of energy, which states that energy can only be transformed, not created or destroyed. The second law of thermodynamics states that even with energy, useful work is not guaranteed; one must account for the intrinsic irreversibility of thermodynamic processes: in the evolution of a thermodynamic system from one equilibrium state to another, entropy never decreases—if the process is reversible, entropy remains unchanged; if irreversible, entropy increases. This is the so-called principle of entropy increase.

The principle of entropy increase governs all naturally occurring processes, driving everything to spontaneously evolve toward maximal disorder. Accordingly, in software development, we can observe a common trajectory in large software products: from order to disorder. As personnel change and requirements shift, adding new features becomes increasingly difficult; fixing bugs often introduces new bugs; no one can clearly explain how to extract a small portion from the system for reuse; and eventually the only option is to scrap and rebuild.

The core of Reversible Computation is reversibility. Its importance lies not only in enabling flexible reuse of software elements; more fundamentally, it reflects the operational laws of our world. By adhering to the principles of Reversible Computation, we can effectively control the rate of entropy increase in the software’s evolutionary process. Ideally, if a system achieves complete reversibility, its entropy remains constant.

In software architecture, we often talk about high cohesion, low coupling, and separation of concerns. But what counts as high? What counts as low? To what degree should we separate? Reversible Computation offers a more operational standard: separate until components are reversible. For concrete software development, Reversible Computation demands that any added functionality should have a paired inverse cancellation mechanism, and any information entered into the system should be automatically recoverable in reverse.

Reversibility is compositional: if each part is reversible and the composition relations among parts are also reversible, then the entire system is reversible. This property makes it possible to incrementally build large reversible systems. A concrete exemplar is the Command pattern.

Each Command contains a paired set of execute and undo operations; a sequence of Commands can be encapsulated as a BatchCommand whose execute and undo operations are obtained by composing those of its sub-commands.

In the real world, entropy increase is an inescapable fate. But even if we cannot completely eliminate entropy increase, we can choose where it occurs—for example, concentrating entropy increase in the throwaway Delta △. Requirements from specific clients always include many random and contingent factors. Incorporating them into the system inevitably damages the previously unified and balanced system structure. If these contingencies can be concentrated in a disposable Delta △, we can protect the core architecture from erosion. When delivering the system to a new client, we need not carry the previous client’s customizations; we can always start from a low-entropy state.

(ii) The World Picture in Quantum Mechanics
Quantum mechanics is a fundamental theory of the physical universe. Its core problems can be reduced to how operators act on the so-called state vector and drive system evolution.

In 1925, Heisenberg proposed the first formulation of quantum mechanics: matrix mechanics, also known as the Heisenberg picture. In this picture, the state vector ψ is fixed and does not evolve in time, while the observable operator A evolves according to the Heisenberg equation (equation omitted here).

In 1926, Schrödinger proposed the second formulation: wave mechanics, also known as the Schrödinger picture. In this picture, operators are fixed and do not evolve in time, while the state evolves according to the Schrödinger equation (equation omitted here).

Shortly thereafter, Schrödinger proved the equivalence of matrix mechanics and wave mechanics.

In 1927, Dirac proposed the so-called “transformation theory,” leading to the third formulation of quantum mechanics: the interaction picture, also known as the Dirac picture. In this picture, one first splits the system’s Hamiltonian as H = H₀ + V, with H₀ known and V a small perturbation. Then we study how the system’s evolution deviates from the known model; that is, we are concerned with the evolution of the Delta description. In the interaction picture, both the state vector and operators evolve in time (equations omitted here).

All three pictures yield identical observable predictions.

The interaction picture is the one most commonly used by physicists in practice. In fact, there is a dedicated branch of mathematical physics: perturbation theory, which systematically studies how a new model evolves when small perturbations are added to a known model. The vast majority of valuable calculations in theoretical physics are carried out within the framework of perturbation theory.

If we compare quantum mechanics with computer theory, we find an interesting correspondence between the world pictures of quantum mechanics and those of computer theory:

Schrödinger picture <--> Turing machine
Heisenberg picture <--> lambda calculus
Dirac picture <--> Reversible Computation

The Turing machine and the lambda calculus establish the conceptual foundations of the universal computer, addressing the feasibility of computation in theory: why a universal machine can exist to perform mechanized actions and complete all conceivable computations with finite resources. Today, with universal computers ubiquitous, our biggest practical challenge is how to compute efficiently. Improvements in computational efficiency depend on discovering “shortcuts” in computation, which in turn depend on insight into the essence of problems—and such insight is intimately tied to how the problem is represented. Transforming the representation can itself be a way to solve problems, because the transformed representation may make the solution obvious. Reversible Computation, via domain models and Delta descriptions, allows us to focus attention on the new problems to be solved.

(iii) Groups and Structuralism
Since Einstein’s discovery of relativity, most basic principles of modern physics have been built on the concept of groups. For example, special relativity corresponds to the Lorentz group, and quantum electrodynamics corresponds to the U(1) gauge group.

A group is a non-empty set G equipped with a binary operation (denote it by +) that satisfies the following four axioms:

Closure. The operation on any two elements of the set yields a result that remains in the set.
Associativity. Operations can be performed locally; the final result is independent of how the operations are associated.
Identity. There exists an identity element 0 in the set that acts trivially on any element.
Inverse. For every element a in the set, there exists an inverse element −a that completely cancels the effect of a.

These four axioms appear simple, yet they encode extremely deep mathematics. For example, even if we restrict to finite groups, without adding any further assumptions and relying solely on these four axioms, one can derive a wealth of theorems. From 1955 to 2004, more than 100 mathematicians wrote over ten thousand pages across more than 500 journal articles to complete the classification of finite groups.

Closure ensures that all results of operations remain within the domain of definition, preventing outcomes that cannot be described; thus operations can continue indefinitely, forming long chains of reasoning. For instance, Lisp exhibits homoiconicity: the output of a Lisp macro is still valid Lisp. This mirrors closure: the macro’s output remains within the same language.

Associativity means we can compute locally without relying on global structure, making secondary encapsulation of substructures possible. For an associative operation, B and C can be abstracted into a new structure X, D and E into a new structure Y, and the combination of X and Y can be abstracted again into structure Z. Associative operations also enable local optimization: by choosing parentheses, we can precompute locally—for example, ((A · B) · C) lets us compute A · B first.

The divide-and-conquer technique often accelerates computation precisely by exploiting associativity—for example, merge sort.

The role of the identity law is that it makes Delta a first-principle concept. In a system with an identity element, any quantity can be regarded as a Delta relative to the identity. With identity e, any a satisfies e + a = a; hence any quantity can be seen as its Delta from e (e.g., x = e + x). Philosophically, physics seeks knowledge by investigating things; all we know are changes in the world, while the world’s noumenon is unobservable. The so-called Delta (positive or negative) is, in fact, the entirety of our knowledge.

If closure, associativity, and identity are satisfied, the structure is a monoid in mathematics. The well-known Monad in functional programming is merely a monoid in the category of endofunctors.

A Monad is only a monoid; it does not satisfy the inverse law. Among the four group axioms, the most difficult to recognize is the inverse law. From the history of number systems, European mathematicians did not accept negative numbers until the 17th century, reflecting the fundamentally perplexing nature of inverses. At that time, people regarded negative numbers as corresponding to nonexistent things—if the corresponding objects do not exist, how can negative numbers themselves exist?

The inverse operation is indispensable for many mathematical derivations. For example, before the introduction of negative numbers, equations like x² + 5x + 6 = 0 versus x² − 5x + 6 = 0 were treated as structurally different and required distinct techniques; without a unified sign concept, it was impossible to abstract a standard form of the quadratic equation. Only with negative numbers can we unify these into the standard ax² + bx + c = 0 form and study general solution techniques.

With inverses, one can relate arbitrary elements and thereby transform between them. This elevates relations among elements beyond simple whole-part or general-particular relationships into a more abstract realm of structural operations. This intellectual shift inspired the structuralist movement in philosophy.

In philosophy, structuralism posits that structure has three essential features: holism, transformability (having rules or laws of transformation), and self-regulation. Holism means a structure cannot be simply partitioned; its components are widely interconnected and transformed via internal relational operations. The whole is a whole precisely because transform/operation is primary. These transformations may be synchronic (among coexisting elements) or diachronic (historical constructive processes), implying that structure always demands an internal constructive process. Independent of the external environment, the structure possesses a kind of self-sufficiency, able to exist independently and maintain internal activity without external conditions. Self-regulation means that internal transformations maintain closure and conservation, ensuring that new components can be generated unboundedly while the structural boundary remains stable. Note that evaluation here does not come from external norms and constraints but is based on the structure’s internal regularities. The emphasis is not on the structure’s adaptability to external conditions, but on the internal completeness of its conceptual system. In fact, a structure that does not directly correspond to the current real-world environment may still have significant value and play an indispensable role in problem solving. From the standpoint of rationality, our concern extends beyond the actual world to all possible worlds. Structure, independent of content, resides in the abstract mathematical realm; a “rational” structure’s value will manifest in any concrete world it fits.

When building domain models, we often ask whether a model accurately describes the business, whether it can accommodate requirement changes, and whether it allows for future extensions. But if we shift our mindset, we find these questions presuppose a uniquely ideal model best suited to the business domain, and we want to know how far the current model deviates from that ideal. These are questions aimed at the ultimately established model, yet during the process of model construction, what are the models that can be leveraged—those that already exist or could exist? Each information model corresponds to an automated reasoning engine that can receive information and perform certain derivations and syntheses. A practical question is how to use existing information more effectively for derivation, how to eliminate redundancy, and how to reduce various transformation costs. We often observe that a particular organization of information more fully uncovers intrinsic associations among information—its use of information is nonlocal, appearing in multiple mutually entangled ways that resist decomposition. Such intrinsic associations are sufficiently rich that we can understand them independently of external factors. These entangled blocks of information naturally become the objects of our modeling.

If we no longer insist on a single, monolithic model, and “coverage capability” is no longer the focus, the modeling pattern undergoes a profound shift: the final model can be woven from a series of micro-models. To some extent, each micro-model is self-sufficient and self-consistent, with its rationality grounded in the coherence of its internal logic. In this pattern, a so-called model is essentially akin to an established mathematical theorem. If we add new hypotheses, we can derive new theorems from existing ones—i.e., establish new models.
<!-- SOURCE_MD5:224622c3c285236b82771ae1e7f8377d-->
