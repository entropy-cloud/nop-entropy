<?xml version="1.0" encoding="UTF-8" ?>

<!--
@rateLimit 为避免调用服务过于频繁，通过rateLimit指定每秒最多允许多少次请求。如果超过则会排队等待。
@logMessage 如果设置为true，则会打印出所有请求和响应消息
-->
<llm x:schema="/nop/schema/xdef.xdef" xmlns:x="/nop/schema/xdsl.xdef"
     xmlns:xdef="/nop/schema/xdef.xdef"
     xdef:name="LlmModel" xdef:bean-package="io.nop.ai.core.model"
     apiStyle="enum:io.nop.ai.core.model.ApiStyle"
     defaultModel="string" logMessage="!boolean=true"
     rateLimit="double" defaultRequestTimeout="long"
     supportToolCalls="!boolean" apiKeyHeader="string"
>

    <!--
    大模型服务所支持的模型列表。通过defaultModel来指定缺省使用的模型
    -->
    <supportModels>csv-set</supportModels>

    <aliasMap>string-map</aliasMap>

    <models xdef:body-type="list" xdef:key-attr="name">
        <model name="!string" xdef:name="LlmModelModel" maxTokensLimit="int" defaultMaxTokens="int" contextLenth="int"
               enableThinkingPrompt="string" disableThinkingPrompt="string"
               thinkStartMarker="string" thinkEndMarker="string" supportToolCalls="boolean"/>
    </models>

    <!--
    服务的基础url，比如http://localhost:11342
    -->
    <baseUrl>string</baseUrl>

    <!--
    聊天功能的服务端点，比如 /api/chat
    -->
    <chatUrl>string</chatUrl>

    <!--
    单次生成服务断点，比如 /api/generate
    -->
    <generateUrl>string</generateUrl>

    <embedUrl>string</embedUrl>

    <request xdef:name="LlmRequestModel"
             seedPath="prop-path"
             topKPath="prop-path"
             topPPath="prop-path"
             temperaturePath="prop-path"
             maxTokensPath="prop-path"
             stopPath="prop-path"
             contextLengthPath="prop-path"
             thinkingPath="prop-path"

    />

    <response xdef:name="LlmResponseModel" xdef:mandatory="true"
              rolePath="prop-path"
              contentPath="!prop-path"
              promptTokensPath="prop-path"
              totalTokensPath="prop-path"
              completionTokensPath="prop-path"
              promptCacheHitTokensPath="prop-path"
              promptCacheCreationTokensPath="prop-path"
              reasoningContentPath="prop-path"
              errorPath="prop-path"
              statusPath="prop-path"
              toolCallsPath="prop-path"
    />

    <buildHttpRequest>xpl-fn:(httpRequest,chatRequest,chatOptions)=>void</buildHttpRequest>

    <parseHttpResponse>xpl-fn:(httpResponse,chatResponse, chatOptions)=>void</parseHttpResponse>

</llm>