<llm x:schema="/nop/schema/ai/llm.xdef" xmlns:x="/nop/schema/xdsl.xdef"
     apiStyle="openai">

    <!--    <baseUrl>http://localhost:11434/</baseUrl>-->
    <chatUrl>/chat/completions</chatUrl>

    <request seedPath="options.seed"
             topPPath="top_p"
             temperaturePath="temperature"
             stopPath="stop"
             maxTokensPath="max_tokens"
    />


    <response contentPath="choices.0.message.content"
              rolePath="choices.0.message.role"
              reasoningContentPath="choices.0.message.reasoning_content"
              promptTokensPath="usage.prompt_tokens"
              completionTokensPath="usage.completion_tokens"
              totalTokensPath="usage.total_tokens"
              statusPath="done"
              errorPath="error"
    />

</llm>